{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koalpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U torch transformers tokenizers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 28/28 [00:01<00:00, 16.75it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 6.00 GiB total capacity; 443.07 MiB already allocated; 4.56 GiB free; 446.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      8\u001b[0m MODEL \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbeomi/KoAlpaca-Polyglot-12.8B\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     11\u001b[0m     MODEL,\n\u001b[0;32m     12\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[0;32m     13\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     14\u001b[0m )\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m, non_blocking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     15\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     17\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m     18\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     19\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     tokenizer\u001b[39m=\u001b[39mMODEL,\n\u001b[0;32m     21\u001b[0m     device\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\transformers\\modeling_utils.py:1902\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1898\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1899\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1900\u001b[0m     )\n\u001b[0;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 6.00 GiB total capacity; 443.07 MiB already allocated; 4.56 GiB free; 446.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import pipeline, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "MODEL = 'beomi/KoAlpaca-Polyglot-12.8B'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device=f\"cuda\", non_blocking=True)\n",
    "model.eval()\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=MODEL,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "def ask(x, context='', is_input_full=False):\n",
    "    ans = pipe(\n",
    "        f\"### 질문: {x}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {x}\\n\\n### 답변:\", \n",
    "        do_sample=True, \n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(ans[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Koalpaca 로딩 시간 : \",end-start)\n",
    "\n",
    "\n",
    "ask(\"딥러닝이 뭐야?\")\n",
    "# 딥러닝은 인공신경망을 통해 입력과 출력 사이의 복잡한 관계를 학습하는 머신러닝의 한 분야입니다. \n",
    "# 이 기술은 컴퓨터가 인간의 학습 능력과 유사한 방식으로 패턴을 학습하도록 하며, 인간의 개입 없이도 데이터를 처리할 수 있는 기술입니다. \n",
    "# 최근에는 딥러닝을 활용한 인공지능 애플리케이션이 많이 개발되고 있습니다. 예를 들어, 의료 진단 애플리케이션에서는 딥러닝 기술을 활용하여 환자의 특징을 파악하고, \n",
    "# 이를 통해 빠르고 정확한 진단을 내리는 데 사용됩니다. 또한, 금융 분야에서는 딥러닝 기술을 활용하여 주가 예측 모형을 학습하는 데 사용되기도 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ask(\u001b[39m\"\u001b[39m\u001b[39m요즘 날씨 너무 춥지 않아?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ask' is not defined"
     ]
    }
   ],
   "source": [
    "ask(\"요즘 날씨 너무 춥지 않아?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 운영시간은 오전 9시부터 오후 6시까지입니다.\n",
      "2. 매장은 동일한 의미의 문장 10개를 나열합니다.\n",
      "\n",
      "위와 같은 간단한 문장들이 있습니다. 그러나 이 문장들은 더 세부적이고 전문적인 내용을 포함할 수 있습니다. 이러한 문장들을 작성할 때는 한 개의 단어를 길게 설명하는 대신, 여러 개의 단어를 포함하는 짧은 문장을 사용하는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "ask(\"운영시간은 오전 9시부터 오후6시까지입니다. 와 동일한 의미의 문장 10개만 만들어줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02:20:45부터 03:20:45까지입니다.\n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"한 문장에 대해 동일한 의미의 문장 10개만 만들어주세요\n",
    "    \n",
    "    문장은 다음과 같습니다.\n",
    "\n",
    "    1.운영시간은 오전 9시부터 오후6시까지입니다. \n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 김밥은 하나에 1500원입니다.\n",
      "2. 과자 한 봉지에 500원입니다.\n",
      "3. 영화 한 편에 5,000원입니다.\n",
      "4. 지하철 한 구간에 500원입니다.\n",
      "5. 제주도 여행에는 비행기 값이 1만 원입니다.\n",
      "6. \"서울의 어느 곳에서도 볼 수 없는 경치입니다.\"는 하나의 문장입니다.\n",
      "7. 목욕탕의 가격은 맥반석에 따라 다릅니다.\n",
      "8. 호텔의 가격은 매우 비싸며, 예약하기 어려울 정도입니다.\n",
      "9. 마트에서 파는 과일의 가격은 매우 저렴합니다.\n",
      "10. 학교의 소풍 비용은 1인당 만 원입니다.\n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"한 문장에 대해 동일한 의미의 문장 10개만 만들어주세요\n",
    "    \n",
    "    문장은 다음과 같습니다.\n",
    "\n",
    "    1.김밥은 하나에 1500원입니다. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 여자친구와 이별했을 때의 감정을 표현하는 10개의 문장입니다.\n",
      "1. \"너무 슬퍼요\"는 그 감정의 정도를 나타내며, 여자친구와의 관계에 대한 솔직한 표현입니다.\n",
      "2. \"내 마음속에는 네가 가득해요\"는 서로 사랑했던 순간과 추억을 떠올리며 다시 만나고 싶은 마음을 표현하는 문장입니다.\n",
      "3. \"가지마요\"는 이제 이별을 받아들이기 어렵지만, 그래도 이별은 싫다는 감정을 표현하는 문장입니다.\n",
      "4. \"내가 더 사랑해\"는 상대방을 이제는 보낼 수 없다는 결심을 하며 이별을 극복하겠다는 의지를 나타내는 문장입니다.\n",
      "5. \"저는 이별에 대한 아픔을 겪고 있어요\"는 치유받고 싶은 마음과 함께, 이별을 극복하겠다는 의지를 나타내는 문장입니다.\n",
      "6. \"저도 가끔은 이별을 원해요\"는 외로워하고 슬퍼하는 감정을 표현하는 문장입니다.\n",
      "7. \"다른 사람을 만나도 괜찮아요\"는 외로움을 느끼는 상황에서 벗어나 새로운 사람을 만나고 싶은 마음을 표현하는 문장입니다.\n",
      "8. \"당신이 내 곁에 있으면 나는 행복해요\"는 상대방을 열렬히 사랑했던 때와 그 감정을 떠올리며 다시 만나고 싶은 마음을 표현하는 문장입니다.\n",
      "9. \"너 때문에 힘들어\"는 이별의 고통과 함께 힘들어한다는 감정을 표현하는 문장입니다.\n",
      "10. \"저는 당신과 함께 갈 수 있다면 좋겠어요\"는 이별의 아쉬움과 절망감을 표현하는 문장입니다.\n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"다음 문장에 대해 동일한 의미의 문장 10개만 만들어주세요\\n여자친구랑 헤어져서 슬프겠구나.\\n정답. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 차 세우는 곳에 주차하지 마세요.\n",
      "2. 주차할 장소에 차를 세우지 마세요.\n",
      "3. 차를 길에 주차하지 마세요.\n",
      "4. 불법 주차를 하지 마세요.\n",
      "5. 교차로, 보도 등에서 주차하지 마세요.\n",
      "6. 차를 타지 않은 경우, 차를 주차하지 마세요.\n",
      "7. 건물 주차장에 주차하지 마세요.\n",
      "8. 차를 내린 곳에 주차하지 마세요.\n",
      "9. 불법 주차 차량을 보면, 강력한 조치를 취하세요.\n",
      "10. 다른 사람의 차량을 불법 주차하지 않도록 하세요.\n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"다음 문장에 대해 동일한 의미의 문장 10개만 만들어주세요\\n주차 문의는 경비실로 연락바랍니다.\\n정답. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"점심메뉴로 뭐 먹지?\"\n",
      "2. \"뭘 먹어야 잘 먹었다고 할까?\"\n",
      "3. \"오늘 점심으로 뭐 먹지?\"\n",
      "4. \"점심 때 뭐 먹지?\"\n",
      "5. \"오늘 점보러 가야하나?\"\n",
      "6. \"점심 때 뭐 먹지?\"\n",
      "7. \"오늘 아침 뭐 먹지?\"\n",
      "8. \"점심 때 뭐 먹지?\"\n",
      "9. \"오늘 밤 뭐 먹지?\"\n",
      "10. \"점심 때 뭐 먹지?\"\n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"다음 문장에 대해 동일한 의미의 문장 10개만 만들어주세요\\n점심 메뉴 추천해주세요.\\n정답. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래는 한 문장으로 표현하는 방법입니다.\n",
      "       \n",
      "1. 점심 뭐 먹지?\n",
      "2. 오늘 점심 뭐 먹지?\n",
      "3. 오늘 점심으로 뭐 먹지?\n",
      "4. 오늘 아침으로 뭐 먹지?\n",
      "5. 아침 식사로 뭐 먹지?\n",
      "6. 점심 식사로 뭐 먹지?\n",
      "7. 오늘 저녁 식사로 뭐 먹지?\n",
      "8. 오늘 밤 간식으로 뭐 먹지?\n",
      "9. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "10. 오늘 점심 식사 대용으로 뭐 먹지?\n",
      "11. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "12. 오늘 밤 간식으로 뭐 먹지?\n",
      "13. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "14. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "15. 오늘 밤 간식으로 뭐 먹지?\n",
      "16. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "17. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "18. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "19. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "20. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "21. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "22. 오늘 밤 간식으로 뭐 먹지?\n",
      "23. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "24. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "25. 오늘 밤 간식으로 뭐 먹지?\n",
      "26. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "27. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "28. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "29. 오늘 저녁 식사 대용으로 뭐 먹지?\n",
      "30. 오늘 아침 식사 대용으로 뭐 먹지?\n",
      "\n",
      "위 예시는 모두 같은 문장이며, 질문자님의 취향에 따라 다르게 표현하거나 추가할 수 있습니다. \n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"다음 문장에 대해 동일한 의미의 문장 30개만 만들어주세요\\n점심 메뉴 추천해주세요.\\n정답. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\koalpaca\\lib\\site-packages\\transformers\\pipelines\\base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음은 점심 메뉴에 대한 몇 가지 추천입니다.\n",
      "\n",
      "1. 김치볶음밥: 어디서든 어떤 메뉴로도 맛있게 만들 수 있는 메뉴입니다. 하지만 김치를 잘게 썰고 기름에 볶은 후 밥과 함께 먹으면 더욱 맛있습니다.\n",
      "\n",
      "2. 제육볶음: 제육을 볶으면서 야채와 함께 양념장을 넣어 끓여 먹으면 매콤한 맛을 즐길 수 있습니다.\n",
      "\n",
      "3. 두부김치: 두부와 김치를 볶아 먹는 메뉴로, 막걸리와 어울리는 안주나 밥 반찬으로도 좋습니다.\n",
      "\n",
      "4. 계란볶음밥: 계란과 파, 김치, 간장 등으로 간단하게 만들 수 있는 메뉴입니다.\n",
      "\n",
      "5. 김치찌개: 돼지고기, 김치, 양파 등으로 넣어 푹 끓인 김치찌개는 국물이 진하고 얼큰해 밥과 함께 먹으면 좋습니다.\n",
      "\n",
      "6. 된장찌개: 된장과 두부, 야채 등을 넣어 끓여 먹는 된장찌개는 한국 사람들이 가장 좋아하는 음식 중 하나입니다.\n",
      "\n",
      "7. 두부비빔밥: 두부와 야채, 김치, 간장 등으로 만든 비빔밥으로, 새콤한 초고추장과 함께 먹으면 더욱 맛있습니다.\n",
      "\n",
      "8. 라면: 라면 국물과 함께 야채, 계란, 김치 등을 넣어 끓인 후, 밥과 함께 먹으면 한 끼 식사로도 충분합니다.\n",
      "\n",
      "9. 카레: 카레는 밥과 야채, 고기 등을 볶아 끓인 후, 카레가루를 넣어 먹습니다. 일본의 대표적인 카레 메뉴인 '치킨커리'도 있습니다.\n",
      "\n",
      "10. 볶음밥: 고추장, 참치, 김치, 깻잎 등으로 볶아 만든 볶음밥은 간단하면서도 맛있는 한 끼 식사입니다.\n",
      "\n",
      "위 추천 메뉴를 참고하여 맛있는 식사를 만들어보세요. \n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"다음 문장에 대한 답변을 10개만 만들어주세요\\n점심 메뉴 추천해주세요.\\n정답. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ask(\"\"\"다음 문장에 대한 답변을 10개만 만들어주세요\\n활동적인 취미 추천좀해주세요.\\n정답. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koalpaca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

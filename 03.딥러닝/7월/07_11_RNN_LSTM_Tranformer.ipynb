{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMzwHwDTAyao2xmPHQ0X8e4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# RNN (순환신경망)"],"metadata":{"id":"eCYJx4EAIris"}},{"cell_type":"code","source":["# cnn은 input이 들어가면 ouput이 나옴\n","# rnn은 이 과정이 순환됌"],"metadata":{"id":"BJK-H7zoKlFu","executionInfo":{"status":"ok","timestamp":1689049018659,"user_tz":-540,"elapsed":3,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6rpB3F-SIjtH","executionInfo":{"status":"ok","timestamp":1689049019292,"user_tz":-540,"elapsed":3,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"outputs":[],"source":["# 비전의 CNN이 있다면 자연어처리에서는 RNN\n","# RNN - LSTM - Seq2Seq - Attension - Transformer - GPT-1 - BERT - GPT-3\n","# GPT-2정도까지는 코랩에서 돌릴 수 있음"]},{"cell_type":"code","source":["# 시계열 데이터 : 시간에 흐름에 따라 정리된 데이터 ex) 날씨,주식 등\n","# 언어도 시계열 데이터임. 단어뒤에 단어들이 나열됌"],"metadata":{"id":"66awe_fmI1lX","executionInfo":{"status":"ok","timestamp":1689049137751,"user_tz":-540,"elapsed":4,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# CNN은 특징(feature)을 뽑을 때 filter를 썼음, 공간의 특성을 활용함\n","# RNN은 특징(feature)를 뽑을 때 전후 관계를 분석함\n","\n","# 전통적인 neural network는 지속되는 생각을 하지 못한다는 것이 큰 단점\n","# RNN은 이 문제를 해결할 수 있음\n","\n","# ex) I am a boy\n","# I가 나온다음 am을 출력하기 위해 I를 기반으로 예측함\n","\n","# 단 문장이 길면 길수록 기울기 소실이 생겨 앞에 나온 단어들의 특징이 소멸됌\n","# ex) hello\n","# ['h','e','l','o']\n","# [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]] (원핫인코딩)\n","# input = [1,0,0,0]  ,output = [0,1,0,0]\n","# input = [0,1,0,0]  ,output = [0,0,1,0] ....\n","# 단어 길이가 작을 때는 효과적임"],"metadata":{"id":"lYlI5XsCLH5T","executionInfo":{"status":"ok","timestamp":1689049860996,"user_tz":-540,"elapsed":480,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# LSTM (Long-Short-Term-Memory)\n","\n","# 과거의 정보(앞에 있는 단어)를 기억하게 됌\n","# LSTM forget gate layer : 과거 정보를 기억할지 잊을지 결정하는 단계 , sigmoid(0~1)에 의해 결정, 0에 가까우면 잊고 1에 가까우면 기억함\n","# LSTM input gate layer : 현재 정보를 기억하기 위한 gate\n","# LSTM cell state : 이전 단계에서 계산된 것을 실행\n","# LSTM output gate layer : 무엇을 output으로 내보낼지 정하는 단계"],"metadata":{"id":"pGm73YHlLnj_","executionInfo":{"status":"ok","timestamp":1689050253739,"user_tz":-540,"elapsed":1054,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# GRU (Gated Recurrent Unit)"],"metadata":{"id":"0UGOnnv7N4Yw","executionInfo":{"status":"ok","timestamp":1689050448407,"user_tz":-540,"elapsed":1,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Seq2Seq\n","\n","# ex) 번역기\n","#  h1   h2   h3                 y1   y2   y3\n","# LSTM LSTM LSTM latent vector LSTM LSTM LSTM\n","#  x1   x2   x3                  x1  x2   x3\n","\n","# latent vector에 많은 정보를 넣었지만 모든 데이터를 압축하다보니 나오는 번역이 이상해짐\n","# 하드웨어의 발전으로 넓은 latent vector으로 해결할 수 있었음"],"metadata":{"id":"ISDuMLXUQH8_","executionInfo":{"status":"ok","timestamp":1689051345452,"user_tz":-540,"elapsed":4,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Attention\n","\n","# * 앞에 나오는 단어들을 통해 뒤에 나오는 값을 유추하자\n","# 시퀀스 가운데 중요한 부분만 집중하자\n","# latent vector가 없음(압축할 필요없이 모든 정보를 다 사용함)\n","# dot product : 벡터들의 내적을 구하여 핵심적인 단어를 찾음\n","# Attention_distribution = softmax(a1,a2,a3,a4)\n","# Attention_value : sum(a1,a2,a3,a4)\n","# tanh"],"metadata":{"id":"8E54NpfyQLGt","executionInfo":{"status":"ok","timestamp":1689051343109,"user_tz":-540,"elapsed":3,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Transformer\n","\n","# encoder ,decoder (여러개의 encoder,decoder가 있음)\n","# multi-head-attention\n","\n","# ex) 번역\n","# 문장1 - encoder - 문장1\n","#       - decoder - 번역된문장\n","\n","# 단어가 들어가면 임베딩되고\n","# Positional encoding되서 들어감\n","# 한 단어와 멀어질 수록 값이 커짐\n","\n","# self-attention 통과\n","# 한 문장 내에서 각 단어별로 다른 단어와 어떤 연관이 있는지 나타냄\n","\n","# decoder\n","# Masked Multo Head Attention\n","# Linear\n","# Softmax\n","\n","# 핵심\n","# encoder 부분 : 단어가 들어오면 임베딩, positional 임베딩\n","# encoder 6번을 거치고 self attention을 거친다\n","# query, key, value값\n","# FC거쳐 최종값\n","# 이 최종값이 decoder에서 나갈때 사용된다"],"metadata":{"id":"35Zm2EyrTZTd","executionInfo":{"status":"ok","timestamp":1689054153094,"user_tz":-540,"elapsed":4,"user":{"displayName":"임정민","userId":"18198802387030959084"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vO3VdbdXaj3T"},"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 도로 상황 분류 프로젝트"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드\n",
    "\n",
    "* youtube 데이터 15000개 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가로로 절반으로 자르는 함수 정의\n",
    "def crop_horizontal_half(img):\n",
    "    width, height = img.size\n",
    "\n",
    "    cropped_img = img.crop((0, height//2, width,height))\n",
    "    # cropped_img = img.crop((0, 0, width,height//2))\n",
    "    return cropped_img\n",
    "\n",
    "# 2/3로 자르는 함수 정의\n",
    "def crop_two_thirds(img):\n",
    "    width, height = img.size\n",
    "    cropped_img = img.crop((0, height*1//3, width,height))\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "\n",
    "transform_youtube = transforms.Compose(\n",
    "    [   \n",
    "        transforms.Lambda(crop_horizontal_half),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_ai_hub = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(crop_horizontal_half),\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "youtube_datasets = torchvision.datasets.ImageFolder(root='D:\\Weather conditions driving classification data (YouTube)\\images',transform=transform_youtube)\n",
    "\n",
    "ai_hub_datasets = torchvision.datasets.ImageFolder(root='D:\\Weather conditions driving classification data (AI hub)\\images',transform=transform_ai_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 15000\n",
       "    Root location: D:\\Weather conditions driving classification data (YouTube)\\images\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Lambda()\n",
       "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.ConcatDataset at 0x18a38098288>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import utils\n",
    "# datasets = utils.data.ConcatDataset([youtube_datasets,ai_hub_datasets])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i,(img,label) in enumerate(ai_hub_datasets):\n",
    "    if i <= 10000:\n",
    "        print(i)\n",
    "        continue\n",
    "    print(i)\n",
    "    img = np.array(img)\n",
    "    img = img.transpose((1,2,0))\n",
    "    # img = Image.OPEN(img)\n",
    "    print(img.shape)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    # print(img)\n",
    "    # print(label)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "randperm() received an invalid combination of arguments - got (float, generator=torch._C.Generator), but expected one of:\n * (int n, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int n, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16672\\2172366095.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_datasets\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtest_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mai_hub_datasets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdatasets_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatasets_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\user\\miniconda3\\envs\\DL_06_22\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[1;34m(dataset, lengths, generator)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSubset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: randperm() received an invalid combination of arguments - got (float, generator=torch._C.Generator), but expected one of:\n * (int n, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int n, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "datasets_size = len(ai_hub_datasets)\n",
    "print(datasets_size)\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_datasets , test_datasets = random_split(ai_hub_datasets,[datasets_size*0.8,datasets_size*0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_06_22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
